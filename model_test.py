import asyncio
from openai import AsyncOpenAI
from dotenv import load_dotenv
import os
import time

load_dotenv()

# ================== è¿™é‡Œæ”¹æˆä½ çš„é…ç½® ==================
BASE_URL = os.getenv("BASE_URL")
API_KEY = os.getenv("API_KEY")

MODEL_LIST = os.getenv("MODEL_LIST")                 # ä½ çš„æ¨¡å‹åˆ—è¡¨æ–‡ä»¶ï¼Œä¸€è¡Œä¸€ä¸ªæ¨¡å‹å
CONCURRENCY = os.getenv("CONCURRENCY")                        # å¹¶å‘æ•°æŠ¥429å°±è°ƒä½

TEST_PROMPT = os.getenv("TEST_PROMPT")
# ====================================================

async def test_model(client: AsyncOpenAI, model: str, semaphore: asyncio.Semaphore):
    """æµ‹è¯•å•ä¸ªæ¨¡å‹æ˜¯å¦å¯ç”¨"""
    async with semaphore:
        try:
            start_time = time.time()
            response = await client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": TEST_PROMPT}],
                max_tokens=20,
                timeout=60,  # å•ä¸ªè¯·æ±‚è¶…æ—¶60ç§’ï¼Œç¾¤å‹å‘æ¶ˆæ¯ï¼Œä¸æ˜¯ç§’å›æ‰æ›´åƒçœŸäººï¼ˆï¼Ÿï¼‰
            )
            # åªè¦èƒ½è¿”å›å“åº”å°±ç®—æˆåŠŸ
            latency = round((time.time() - start_time) * 1000, 2)
            print(f"âœ… {model.ljust(40)} å¯ç”¨ | å»¶è¿Ÿ: {latency}ms")
            # é¡ºä¾¿çœ‹çœ‹çŒ«å¨˜å›åº”ï¼ˆï¼Ÿï¼‰
            print(response.choices[0].message.content)
            return model, True
        except Exception as e:
            error_msg = str(e).replace("\n", " ").strip()
            if "rate limit" in error_msg.lower():
                print(f"âš ï¸ {model.ljust(40)} é™æµ")
            elif "invalid" in error_msg.lower() or "not found" in error_msg.lower():
                print(f"âŒ {model.ljust(40)} ä¸å­˜åœ¨æˆ–ä¸å¯ç”¨")
            else:
                print(f"âŒ {model.ljust(40)} å¤±è´¥: {error_msg}")
            return model, False


async def main():
    # è¯»å–æ¨¡å‹åˆ—è¡¨
    try:
        with open(MODEL_LIST, "r", encoding="utf-8") as f:
            models = [line.strip() for line in f if line.strip() and not line.startswith("#")]
        print(f"ğŸ“‹ è¯»å–åˆ° {len(models)} ä¸ªæ¨¡å‹ï¼Œå¼€å§‹æµ‹è¯•...\n")
    except FileNotFoundError:
        print(f"âŒ æœªæ‰¾åˆ° {MODEL_LIST} æ–‡ä»¶ï¼")
        return

    if not models:
        print("âš ï¸ æ¨¡å‹åˆ—è¡¨ä¸ºç©ºï¼Œé€€å‡ºã€‚")
        return

    # åˆ›å»ºå®¢æˆ·ç«¯
    client = AsyncOpenAI(
        api_key=API_KEY,
        base_url=BASE_URL,
        timeout=20,
    )

    # æ§åˆ¶å¹¶å‘
    semaphore = asyncio.Semaphore(CONCURRENCY)

    # å¹¶å‘æµ‹è¯•æ‰€æœ‰æ¨¡å‹
    tasks = [test_model(client, model, semaphore) for model in models]
    results = await asyncio.gather(*tasks)

    # æ”¶é›†å¯ç”¨çš„æ¨¡å‹
    alive_models = [model for model, success in results if success]

    # å†™å…¥å¯ç”¨æ¨¡å‹æ–‡ä»¶
    with open("alive_models.txt", "w", encoding="utf-8") as f:
        for m in alive_models:
            f.write(m + "\n")

    print("\n" + "="*60)
    print(f"ğŸ‰ æµ‹è¯•å®Œæˆï¼å…±æµ‹è¯• {len(models)} ä¸ªæ¨¡å‹ï¼Œå¯ç”¨ {len(alive_models)} ä¸ª")
    print(f"ğŸ’¾ å¯ç”¨æ¨¡å‹å·²ä¿å­˜è‡³ alive_models.txt")
    if alive_models:
        print("\nå¯ç”¨æ¨¡å‹åˆ—è¡¨ï¼š")
        for m in alive_models:
            print(f"    â€¢ {m}")

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹æ‰¹é‡æµ‹è¯•APIæ¨¡å‹å¯ç”¨æ€§...\n")
    asyncio.run(main())